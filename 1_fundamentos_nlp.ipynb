{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.fundamentos-nlp.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtig0Rn0knexhwcf9uWgi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francomanca93/fundamentos-NLP-con-python-y-NLTK/blob/main/1_fundamentos_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHgtlEVHVv62"
      },
      "source": [
        "# Inicio a los fundamentos | Configurar ambiente de trabajo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzIyiPgkNPuG",
        "outputId": "f5c9deee-d6dc-427e-85cd-2a78f14a2cce"
      },
      "source": [
        "import nltk\n",
        "nltk.download('cess_esp')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hhteHPgObmW",
        "outputId": "a5a2b636-ca83-409b-92a9-4b19d02d7a5e"
      },
      "source": [
        "#Importamos una biblioteca para expresiones regulares. \n",
        "import re\n",
        "# Definimos un corpus en Python \n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "# Vemos de que trata este contenido\n",
        "print(corpus)\n",
        "# Vemos el tamaño del nuestro corpus. \n",
        "print(len(corpus))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccHP72b7RUCn",
        "outputId": "7972db74-cc05-4c8b-9ec9-db49522ac300"
      },
      "source": [
        "# Aquí vamos a concatener todas esas sublistas en una lista grande.\n",
        "# Con esto no tendremos una lista de listas, si no una sola lista donde están\n",
        "# todos los titulares seguidos uno tras otro  \n",
        "flatten = [w for l in corpus for w in l]\n",
        "\n",
        "# [w for l in corpus for w in l] == \n",
        "# \n",
        "# for l in corpus:\n",
        "#   for w in l:\n",
        "\n",
        "# Imprimimos una parte de la estructura interna \n",
        "print(flatten[:100])\n",
        "print(len(flatten))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.', 'Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.', 'La', 'electricidad', 'producida', 'pasará', 'a', 'la', 'red', 'eléctrica', 'pública', 'de', 'México', 'en_virtud_de', 'un', 'acuerdo', 'de', 'venta']\n",
            "192685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRXFHgzDPdeF",
        "outputId": "33fb0d8d-df5d-46da-c4c8-b0ecb0e94147"
      },
      "source": [
        "print(flatten[:50])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.', 'Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koXQXLMvQpUP"
      },
      "source": [
        "# Expresiones Regulares\n",
        "\n",
        "\n",
        "*   Constituyen un lenguaje estandarizado para definir cadenas de búsqueda de texto.\n",
        "*   Libreria de operaciones con  expresiones regulares de Python [re](https://docs.python.org/3/library/re.html)\n",
        "*   Reglas para escribir expresiones regulares [Wiki](https://es.wikipedia.org/wiki/Expresión_regular)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhw8C8aV8Pj"
      },
      "source": [
        "## Palabras, textos y vocabularios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHvZ-7uGWjja"
      },
      "source": [
        "Estructura de la funcion re.search(): Determina si el patron de búsqueda p esta contenido en la cadena s `re.seach(p, s)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia8j5Tm8WSKu",
        "outputId": "64f4e0ad-ef10-4c80-823e-77fef54b17ba"
      },
      "source": [
        "arr = [w for w in flatten if re.search('es', w)]\n",
        "print(arr[:5])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1NOvEg_gWfT"
      },
      "source": [
        "#### Metacaracteres ^ y $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OirTIU-8WvcV",
        "outputId": "8f0e0cf7-c545-4147-e5fd-f28d593ff45a"
      },
      "source": [
        "# El simbolo $ nos indica que lo que busquemos este al final\n",
        "arr = [w for w in flatten if re.search('es$', w)]\n",
        "print(arr[:5])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jueves', 'centrales', 'millones', 'millones', 'dólares']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSKS7V9HXIpF",
        "outputId": "f03853f0-1814-4c70-b8b4-7469a893ddb8"
      },
      "source": [
        "# El simbolo ^ nos indica que lo que busquemos este al inicio\n",
        "arr = [w for w in flatten if re.search('^es', w)]  \n",
        "print(arr[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['estatal', 'es', 'esta', 'esta', 'eso']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIb05pdjgflg"
      },
      "source": [
        "#### Rangos [a-z] y [...]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtVsVj-CYI4G",
        "outputId": "d9c873a2-5ac9-42d0-cd06-9527f127ad43"
      },
      "source": [
        "# Rango [a-z] = Cualquier caracter que este ubicado entre a-z\n",
        "# Rango [ghi] = Cualquier caracter que sea [ghi]\n",
        "\n",
        "arr = [w for w in flatten if re.search('^[ghi]', w)]  \n",
        "print(arr[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['grupo', 'hoy', 'gas', 'gas', 'intervendrá', 'invertir', 'gas', 'hoy', 'insulto', 'intervención']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ94PIZygizQ"
      },
      "source": [
        "#### Clausuras * y +"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4st1-ajYnWu",
        "outputId": "b993649b-0ac3-4e63-c5df-d70241eb8559"
      },
      "source": [
        "# Clausura\n",
        "# * Una cadena de texto se puede repetir 0 o mas veces\n",
        "# + Una cadena de texto se puede repetir 1 o mas veces\n",
        "arr = [w for w in flatten if re.search('^(no)*', w)]  \n",
        "print('Clausura *')\n",
        "print(arr[:10])\n",
        "\n",
        "arr = [w for w in flatten if re.search('^(no)+', w)]  \n",
        "print('Clausura +')\n",
        "print(arr[:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clausura *\n",
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',']\n",
            "Clausura +\n",
            "['norte', 'no', 'no', 'noche', 'no', 'no', 'notificación', 'no', 'no', 'no']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY6Ya9HVg8HI"
      },
      "source": [
        "# Normalización de textos (como aplicación de regex)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7IWzaRphpaK",
        "outputId": "c0439ec5-6c76-435f-ba31-9db0cb8220a7"
      },
      "source": [
        "# Intepretacion str como texto plano\n",
        "print(r'esta es una prueba \\n una prueba')\n",
        "print('*'*64)\n",
        "# Intepretacion str con caracteres especiales, ej: \\n\n",
        "print('esta es una prueba \\n una prueba')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esta es una prueba \\n una prueba\n",
            "****************************************************************\n",
            "esta es una prueba \n",
            " una prueba\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwhyb4bZg1AG"
      },
      "source": [
        "## Tokenizacion con Expresiones Regulares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrqmFqzehCdb"
      },
      "source": [
        "Aplicaremos las expresiones regulares para formar algoritmos de tokenización.\n",
        "\n",
        "Tokenización es el proceso mediante el cual se sub-divide un cadena de texto en unidades linguisticas minimas (palabras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgF_EmB0iH39"
      },
      "source": [
        "texto = \"\"\" Cuando sea el rey del mundo (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. \n",
        "            Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera \n",
        "            visualizar en su cabeza ...\"\"\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3TaL2CiiUfz",
        "outputId": "120785a6-94fa-460b-8dad-a4b76e11cef4"
      },
      "source": [
        "# Caso 1: tokenizar por espacios vacios\n",
        "print(re.split(r' ', texto)) # Que separe a \"texto\" con el caracter r' '"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', '', 'preocuparme', 'por', 'estas', 'bobadas.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YBc73aBiqfS",
        "outputId": "bf133c42-60bd-4256-c3bd-4a24d3e8bbf5"
      },
      "source": [
        "# Caso 2: tokenizacion usando regex. Por diferentes tipos de espacios tab, salto, espacio\n",
        "print(re.split(r'[ \\t\\n]+', texto)) # Que separe a \"texto\" con los caracteres en el rango r'[ \\t\\n]+'"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXiccjkRjH4e",
        "outputId": "6e33cc13-a3d7-4372-df1e-05029a606dcf"
      },
      "source": [
        "# Caso 3: tokenizacion usando regex 2. Para diferentes tipos de espacios y caracteres no alfanumericos\n",
        "# El metacaracter \\W lo que hace es hacer match con todo lo que no sea un caracter alfanumérico (como paréntesis, símbolos raros, etc.)\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto)) # Que separe a \"texto\" con los caracteres en el rango r'[ \\W\\t\\n]+'"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtVKLxJ-jl3G",
        "outputId": "c3df7b6d-c0b4-4646-8af5-553cbe35b0a1"
      },
      "source": [
        "# Caso 4: tokenizacion usando regex mas fofisticada\n",
        "#En lugar de usar [\\t\\n]+ para definir un rango de espacios, tabs, etc.\n",
        "#Se puede usar el metacaracter \\s, específicamente diseñado para reconocer diferentes clases de espacios\n",
        "print(re.split(r'[\\W\\s]+', texto)) # Que separe a \"texto\" con los caracteres en el rango r'[\\W\\s]+'"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3LUUznJk9F-"
      },
      "source": [
        "## Tokenizador de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yYV5WFsk_pK",
        "outputId": "8f884be7-ac0c-4826-c671-19e7fe0fae76"
      },
      "source": [
        "# Nuestra expresion regular falta\n",
        "texto = 'En los E.U. esa postal vale $15.50 ...'\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBvlPGDtlIJ7",
        "outputId": "2542cc42-93f7-4791-e90f-ffffca1f4282"
      },
      "source": [
        "pattern = r'''(?x)                  # Flag para iniciar el modo verbose\n",
        "              (?:[A-Z]\\.)+          # Hace match con abreviaciones como U.S.A.\n",
        "              | \\w+(?:-\\w+)*        # Hace match con palabras que pueden tener un guión interno\n",
        "              | \\$?\\d+(?:\\.\\d+)?%?  # Hace match con dinero o porcentajes como $15.5 o 100%\n",
        "              | \\.\\.\\.              # Hace match con puntos suspensivos\n",
        "              | [][.,;\"'?():-_`]    # Hace match con signos de puntuación\n",
        "              '''\n",
        "nltk.regexp_tokenize(texto, pattern)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}